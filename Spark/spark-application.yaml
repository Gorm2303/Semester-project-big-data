apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: pyspark-application
spec:
  version: "1.0"
  # An image which will be deployed to the driver and executor pods which builds on top of the base Stackable Spark image
  sparkImage: docker.stackable.tech/stackable/pyspark-k8s:3.3.0-stackable23.7.0
  # The mode in which the Spark application will be executed
  mode: cluster
  # The location of the program to execute, in this case a Python file which is part of the image specified above
  mainApplicationFile: "s3a://spark-data/python-analysis.py"
  # The argument is the path to the file to be processed
  args:
    - "s3a://spark-data/real_estate.zip"
    - "s3a://spark-data/yelp.zip"
  # The location for the Spark event logs, we will later take a look at this using the Spark History Server
  logFileDirectory:
    s3:
      prefix: eventlogs/
      bucket:
        reference: spark-history
  s3connection:
    # S3 credentials to access the data
    reference: data-connection
  sparkConf:
    spark.hadoop.fs.s3a.aws.credentials.provider: "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    # The uri of the MinIO S3 server
    spark.hadoop.fs.s3a.endpoint: "http://minio:9000"
  driver:
    # The resources which will be allocated to the driver pod
    resources:
      cpu:
        min: "0.1"
        max: "0.2"
      memory:
        limit: "512Mi"
  executor:
    # The amount of executor instances which will be deployed
    instances: 1
    # The resources which will be allocated to each executor pod
    resources:
      cpu:
        min: "0.1"
        max: "0.2"
      memory:
        limit: "512Mi"